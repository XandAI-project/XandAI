# ===========================================
# XandAI - Stable Diffusion Forge (Optional)
# ===========================================
#
# Deploy with: docker compose -f docker-compose.yml -f docker-compose.forge.yml up -d --build
#
# Requirements:
# - NVIDIA GPU with CUDA support (12GB+ VRAM recommended for SDXL)
# - nvidia-container-toolkit installed
# - First run will initialize Forge (~5-10 minutes)
#
# Access Forge WebUI at: http://your-server:7685
# API is available at: http://your-server:7685/sdapi/v1/

services:
  # Stable Diffusion Forge WebUI with SDXL
  forge:
    image: ghcr.io/ai-dock/stable-diffusion-webui-forge:latest
    container_name: xandai-forge
    restart: unless-stopped
    # Fix DNS resolution issues
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    environment:
      # Auto-update on startup
      AUTO_UPDATE: "false"
      # Forge startup arguments - API only mode (no WebUI)
      FORGE_ARGS: "--nowebui --api --listen --xformers"
      # Optional: HuggingFace token for downloading models
      HF_TOKEN: ${HF_TOKEN:-}
      # Optional: Civitai token for downloading models
      CIVITAI_TOKEN: ${CIVITAI_TOKEN:-}
    ports:
      - "7865:17860"
    volumes:
      # Persistent workspace
      - forge_workspace:/workspace
      # Models directory - mount your local models here (correct path with /opt)
      - forge_models:/opt/stable-diffusion-webui-forge/models
      # Outputs directory
      - forge_outputs:/opt/stable-diffusion-webui-forge/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:17860/sdapi/v1/sd-models || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 600s
    networks:
      - xandai-network

  # Override backend to enable SD and connect to Forge
  backend:
    environment:
      # Ollama runs on host machine - MUST use host.docker.internal
      OLLAMA_BASE_URL: http://host.docker.internal:11434
      # Enable Stable Diffusion auto-configuration
      SD_ENABLED: "true"
      SD_BASE_URL: http://forge:17860
      SD_API_USER: ""
      SD_API_PASSWORD: ""
      SD_DEFAULT_MODEL: ${SD_DEFAULT_MODEL:-sd_xl_base_1.0.safetensors}
    depends_on:
      - postgres

volumes:
  forge_workspace:
    driver: local
  forge_models:
    driver: local
  forge_outputs:
    driver: local
